#!/usr/bin/env python3
"""
Daily crypto digest using OpenAI Responses API with Web3 MCP integration
"""
import os
import sys
import logging
import requests
from datetime import datetime, timezone
from typing import Optional, Tuple
from openai import OpenAI

# Configure logging (console only for demo)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Environment variables
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
MODEL = os.getenv("OPENAI_MODEL", "gpt-4")

# MCP Server URL - using working tiktoken MCP for demo
MCP_TIKTOKEN_URL = "https://gitmcp.io/openai/tiktoken"

# Direct Telegram API integration
TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_USER_ID = os.environ["TELEGRAM_USER_ID"]

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

def send_telegram_message(message: str) -> bool:
    """Send message directly to Telegram user via Bot API (DEMO MODE)."""
    try:
        # DEMO MODE: Just log the message instead of sending to Telegram
        logger.info("="*60)
        logger.info("ðŸ“± TELEGRAM MESSAGE (DEMO MODE):")
        logger.info("="*60)
        logger.info(f"TO: User {TELEGRAM_USER_ID}")
        logger.info(f"MESSAGE:\n{message}")
        logger.info("="*60)
        
        logger.info(f"âœ… Message would be sent to Telegram user {TELEGRAM_USER_ID}")
        return True
            
    except Exception as e:
        logger.error(f"Error in demo telegram function: {str(e)}")
        return False

# Remote MCP Tools Configuration
tools = [
    {
        "type": "mcp",
        "server_url": MCP_TIKTOKEN_URL,
        "server_label": "tiktoken",
        "allowed_tools": [
            "fetch_tiktoken_documentation",
            "search_tiktoken_documentation", 
            "search_tiktoken_code",
            "fetch_generic_url_content"
        ],
        "require_approval": "never"
    }
]

# System instructions for tiktoken MCP analysis
SYSTEM_INSTRUCTIONS = """
Ð¢Ñ‹ â€” AI-ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¸ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¸ÑÐ°Ñ‚ÐµÐ»ÑŒ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ tiktoken MCP Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°.

ÐŸÐ ÐžÐ¦Ð•Ð¡Ð¡:
1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ tiktoken MCP Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸:
   - fetch_tiktoken_documentation: Ð¿Ð¾Ð»ÑƒÑ‡Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ
   - search_tiktoken_documentation: Ð½Ð°Ð¹Ð´Ð¸ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ðµ Ð´ÐµÑ‚Ð°Ð»Ð¸  
   - search_tiktoken_code: Ð¸Ð·ÑƒÑ‡Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ ÐºÐ¾Ð´Ð°
   - fetch_generic_url_content: Ð¿Ð¾Ð»ÑƒÑ‡Ð¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹

2. Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¾Ð± AI/Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸

Ð¤ÐžÐ ÐœÐÐ¢ Ð¡Ð’ÐžÐ”ÐšÐ˜:
ðŸ¤– **AI Insights Daily**

â€¢ **Ð¢ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ Ð´Ð½Ñ**: Ñ‡Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ tiktoken Ð¸ Ð·Ð°Ñ‡ÐµÐ¼ Ð½ÑƒÐ¶ÐµÐ½
â€¢ **ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ**: ÐºÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð² GPT Ð¼Ð¾Ð´ÐµÐ»ÑÑ…
â€¢ **Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¹ Ñ„Ð°ÐºÑ‚**: Ð¼Ð°Ð»Ð¾Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸
â€¢ **Ð¡Ð¾Ð²ÐµÑ‚ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÑƒ**: Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ
â€¢ **AI Ñ‚Ñ€ÐµÐ½Ð´**: ÑÐ²ÑÐ·ÑŒ Ñ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
- ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ 1000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ tiktoken MCP
- ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐ·Ñ‹Ðº, Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ð´Ð»Ñ ÑˆÐ¸Ñ€Ð¾ÐºÐ¾Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¸
- Ð”Ð¾Ð±Ð°Ð²ÑŒ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð´Ð»Ñ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ð¾ÑÑ‚Ð¸
- Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÑŒ ÑÐ²Ð¾Ð´ÐºÑƒ Ð² Telegram

ÐÐ°Ñ‡Ð¸Ð½Ð°Ð¹ Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ tiktoken Ñ‡ÐµÑ€ÐµÐ· MCP Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹.
"""

def run_ai_insights() -> Tuple[str, str]:
    """
    Execute daily AI insights using Responses API with tiktoken MCP integration.
    
    Returns:
        Tuple of (response_id, insights_text)
    """
    try:
        logger.info("Starting AI insights generation with tiktoken MCP...")
        current_date = datetime.now().strftime('%d.%m.%Y')
        current_time = datetime.now().strftime('%H:%M')
        
        # Prepare request body with remote MCP tools
        body = {
            "model": MODEL,
            "tools": tools,  # Include remote MCP configuration
            "instructions": SYSTEM_INSTRUCTIONS,
            "input": f"Ð¡Ð¾Ð·Ð´Ð°Ð¹ ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½ÑƒÑŽ AI Insights ÑÐ²Ð¾Ð´ÐºÑƒ Ð·Ð° {current_date}. "
                    f"Ð¢ÐµÐºÑƒÑ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ: {current_time} Europe/Amsterdam. "
                    f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ tiktoken MCP Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, "
                    f"Ð·Ð°Ñ‚ÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ð¹ Ð¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÑŒ ÐµÑ‘ Ð² Telegram.",
        }
        
        # Make API call with MCP tools
        logger.info("Calling OpenAI Responses API with tiktoken MCP...")
        logger.info(f"MCP Server URL: {MCP_TIKTOKEN_URL}")
        logger.info(f"Tools configured: {len(tools[0]['allowed_tools'])} tools available")
        
        response = client.responses.create(**body)
        
        # Extract response data
        response_id = response.id
        insights_text = getattr(response, "output_text", "").strip()
        
        logger.info(f"AI Response completed. ID: {response_id}")
        logger.info(f"Generated insights preview: {insights_text[:200]}...")
        
        # Send insights to Telegram
        if insights_text:
            logger.info("Sending insights to Telegram...")
            telegram_success = send_telegram_message(insights_text)
            
            if telegram_success:
                logger.info("âœ… AI Insights successfully sent to Telegram!")
            else:
                logger.error("âŒ Failed to send insights to Telegram")
                raise Exception("Telegram delivery failed")
        else:
            logger.error("âŒ No insights content generated by AI")
            raise Exception("Empty insights content")
        
        return response_id, insights_text
        
    except Exception as e:
        logger.error(f"Error in run_ai_insights: {str(e)}")
        raise

def run_with_retries(max_retries: int = 3) -> Tuple[str, str]:
    """
    Run AI insights with exponential backoff retry logic.
    
    Args:
        max_retries: Maximum number of retry attempts
        
    Returns:
        Tuple of (response_id, output_text)
    """
    for attempt in range(max_retries + 1):
        try:
            return run_ai_insights()
            
        except Exception as e:
            if attempt == max_retries:
                logger.error(f"All {max_retries + 1} attempts failed. Last error: {str(e)}")
                raise
            
            wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
            logger.warning(f"Attempt {attempt + 1} failed: {str(e)}. Retrying in {wait_time}s...")
            
            import time
            time.sleep(wait_time)

if __name__ == "__main__":
    try:
        logger.info("=== AI Insights Daily Service Started (tiktoken MCP Mode) ===")
        logger.info(f"Model: {MODEL}")
        logger.info(f"tiktoken MCP URL: {MCP_TIKTOKEN_URL}")
        logger.info(f"Timezone: Europe/Amsterdam")
        
        # Run insights generation with retries
        response_id, insights_text = run_with_retries()
        
        logger.info("=== AI Insights completed successfully ===")
        logger.info(f"Final response ID: {response_id}")
        
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        sys.exit(1)
